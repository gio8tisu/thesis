@article{Gauthier2014,
abstract = {We apply an extension of generative adversarial networks (GANs) [8] to a conditional setting. In the GAN framework, a " generator " network is tasked with fooling a " discriminator " network into believing that its own samples are real data. We add the capability for each network to condition on some arbitrary external data which describes the image being generated or discriminated. By varying the conditional information provided to this extended GAN, we can use the resulting generative model to generate faces with specific attributes from nothing but random noise. We evaluate the likelihood of real-world faces under the generative model, and examine how to determinis-tically control face attributes by modifying the conditional information provided to the model.},
author = {Gauthier, Jon},
file = {:Users/sergio/Documents/TFG/related papers/paper.pdf:pdf},
journal = {Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester},
title = {{Conditional generative adversarial nets for convolutional face generation}},
year = {2014}
}
@article{Combalia2019,
abstract = {Specialists have used confocal microscopy in the ex-vivo modality to identify tumors with an overall sensitivity of 96.6{\%} and speciﬁcity of 89.2{\%}. However, this technology hasn't established yet in the standard clinical practice because most pathologists lack the knowledge to interpret its output. In this paper we propose a combination of deep learning and computer vision techniques to digitally stain confocal microscopy images into H{\&}E-like slides, enabling pathologists to interpret these images without speciﬁc training. We use a fully convolutional neural network with a multiplicative residual connection to denoise the confocal microscopy images, and then stain them using a Cycle Consistency Generative Adversarial Network.},
author = {Combalia, Marc and P{\'{e}}rez-Anker, Javiera and Garc{\'{i}}a-Herrera, Adriana and Alos, Ll{\'{u}}cia and Vilaplana, Ver{\'{o}}nica and Marqu{\'{e}}s, Ferra and Puig, Susana and Malvehy, Josep},
file = {:Users/sergio/Documents/TFG/related papers/Digitally Stained Confocal Microscopy through Deep Learning.pdf:pdf;:Users/sergio/Library/Application Support/Mendeley Desktop/Downloaded/Combalia et al. - 2019 - Digitally Stained Confocal Microscopy through Deep Learning.pdf:pdf},
journal = {Proceedings of Machine Learning Research},
keywords = {Confocal Microscopy,CycleGAN,Deep learning,Digital Staining,Neural Networks,Speckle Noise},
pages = {1--9},
title = {{Digitally Stained Confocal Microscopy through Deep Learning}},
year = {2019}
}
@inproceedings{Wang2018a,
abstract = {We propose a novel approach for generating high quality visible-like images from Synthetic Aperture Radar (SAR) images using Deep Convolutional Generative Adversarial Network (GAN) architectures. The proposed approach is based on a cascaded network of convolutional neural nets (CNNs) for despeckling and image colorization. The cascaded structure results in faster convergence during training and produces high quality visible images from the corresponding SAR images. Experimental results on both simulated and real SAR images show that the proposed method can produce visible-like images better compared to the recent state-of-the-art deep learning-based methods.},
author = {Wang, Puyang and Patel, Vishal M},
booktitle = {2018 IEEE Radar Conference, RadarConf 2018},
doi = {10.1109/RADAR.2018.8378622},
file = {:Users/sergio/Documents/TFG/related papers/wang2018.pdf:pdf},
isbn = {9781538641675},
keywords = {Synthetic aperture radar image,colorization,despeckling},
pages = {570--575},
title = {{Generating high quality visible images from SAR images using CNNs}},
year = {2018}
}
@article{Huang2017,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
author = {Huang, Gao and Liu, Zhuang and {Van Der Maaten}, Laurens and Weinberger, Kilian Q.},
doi = {10.1109/CVPR.2017.243},
file = {:Users/sergio/Documents/TFG/related papers/08099726.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {2261--2269},
title = {{Densely connected convolutional networks}},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{Wang2018,
abstract = {We propose a novel approach for generating high quality visible-like images from Synthetic Aperture Radar (SAR) images using Deep Convolutional Generative Adversarial Network (GAN) architectures. The proposed approach is based on a cascaded network of convolutional neural nets (CNNs) for despeckling and image colorization. The cascaded structure results in faster convergence during training and produces high quality visible images from the corresponding SAR images. Experimental results on both simulated and real SAR images show that the proposed method can produce visible-like images better compared to the recent state-of-the-art deep learning-based methods.},
author = {Wang, Puyang and Patel, Vishal M.},
booktitle = {2018 IEEE Radar Conference, RadarConf 2018},
doi = {10.1109/RADAR.2018.8378622},
file = {:Users/sergio/Documents/TFG/related papers/Generating High Quality Visible Images from SAR Images Using CNNs.pdf:pdf},
isbn = {9781538641675},
keywords = {Synthetic aperture radar image,colorization,despeckling},
pages = {570--575},
title = {{Generating high quality visible images from SAR images using CNNs}},
year = {2018}
}
@article{Isola2016,
abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
archivePrefix = {arXiv},
arxivId = {1611.07004},
author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
eprint = {1611.07004},
file = {:Users/sergio/Library/Application Support/Mendeley Desktop/Downloaded/Isola et al. - 2016 - Image-to-Image Translation with Conditional Adversarial Networks.pdf:pdf},
month = {nov},
title = {{Image-to-Image Translation with Conditional Adversarial Networks}},
url = {http://arxiv.org/abs/1611.07004},
year = {2016}
}
@article{Johnson2016,
abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a $\backslash$emph{\{}per-pixel{\}} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing $\backslash$emph{\{}perceptual{\}} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
archivePrefix = {arXiv},
arxivId = {1603.08155},
author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
eprint = {1603.08155},
file = {:Users/sergio/Library/Application Support/Mendeley Desktop/Downloaded/Johnson, Alahi, Fei-Fei - 2016 - Perceptual Losses for Real-Time Style Transfer and Super-Resolution.pdf:pdf},
month = {mar},
title = {{Perceptual Losses for Real-Time Style Transfer and Super-Resolution}},
url = {http://arxiv.org/abs/1603.08155},
year = {2016}
}
@article{DBLP:journals/corr/Goodfellow17,
archivePrefix = {arXiv},
arxivId = {1701.00160},
author = {Goodfellow, Ian J},
eprint = {1701.00160},
journal = {CoRR},
title = {{{\{}NIPS{\}} 2016 Tutorial: Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1701.00160},
volume = {abs/1701.0},
year = {2017}
}
@article{Wang2004,
abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
author = {Wang, Zhou and Bovik, A C and Sheikh, H R and Simoncelli, E P},
doi = {10.1109/TIP.2003.819861},
file = {:Users/sergio/Documents/TFG/related papers/wang03-reprint.pdf:pdf},
issn = {1057-7149 VO - 13},
journal = {IEEE Transactions on Image Processing},
keywords = {Algorithms,Automated,Computer-Assisted,Data Interpretation,Data mining,Degradation,Humans,Hypermedia,Image Enhancement,Image Interpretation,Image quality,Indexes,Information Storage and Retrieval,JPEG,JPEG2000,Layout,Models,Pattern Recognition,Quality Control,Quality assessment,Reproducibility of Results,Sensitivity and Specificity,Signal Processing,Statistical,Subtraction Technique,Transform coding,Visual perception,Visual system,data compression,distorted image,error sensitivity,error visibility,human visual perception,human visual system,image coding,image compression,image database,perceptual image quality assessment,reference image,structural information,structural similarity index,visual perception},
number = {4},
pages = {600--612},
title = {{Image quality assessment: from error visibility to structural similarity}},
url = {http://www.cns.nyu.edu/pub/lcv/wang03-reprint.pdf},
volume = {13},
year = {2004}
}
@article{Shaban2018,
archivePrefix = {arXiv},
arxivId = {1804.01601},
author = {Shaban, M Tarek and Baur, Christoph and Navab, Nassir and Albarqouni, Shadi},
eprint = {1804.01601},
file = {:Users/sergio/Library/Application Support/Mendeley Desktop/Downloaded/Shaban et al. - 2018 - StainGAN Stain Style Transfer for Digital Histological Images.pdf:pdf},
month = {apr},
title = {{StainGAN: Stain Style Transfer for Digital Histological Images}},
url = {https://arxiv.org/abs/1804.01601},
year = {2018}
}
@inproceedings{Li2016,
abstract = {This paper proposes Markovian Generative Adversarial Networks (MGANs), a method for training generative neural networks for efficient texture synthesis. While deep neural network approaches have recently demonstrated remarkable results in terms of synthesis quality, they still come at considerable computational costs (minutes of run-time for low-res images). Our paper addresses this efficiency issue. Instead of a numerical deconvolution in previous work, we precompute a feed-forward, strided convolutional network that captures the feature statistics of Markovian patches and is able to directly generate outputs of arbitrary dimensions. Such network can directly decode brown noise to realistic texture, or photos to artistic paintings. With adversarial training, we obtain quality comparable to recent neural texture synthesis methods. As no optimization is required any longer at generation time, our run-time performance (0.25M pixel images at 25Hz) surpasses previous neural texture synthesizers by a significant margin (at least 500 times faster). We apply this idea to texture synthesis, style transfer, and video stylization.},
archivePrefix = {arXiv},
arxivId = {arXiv:1604.04382v1},
author = {Li, Chuan and Wand, Michael},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46487-9_43},
eprint = {arXiv:1604.04382v1},
file = {:Users/sergio/Documents/TFG/related papers/1604.04382.pdf:pdf},
isbn = {9783319464862},
issn = {16113349},
keywords = {Adversarial generative networks,Texture synthesis,adversarial generative networks,texture synthesis},
pages = {1--17},
title = {{Precomputed real-time texture synthesis with markovian generative adversarial networks}},
url = {https://arxiv.org/abs/1604.04382},
year = {2016}
}
@article{DBLP:journals/corr/abs-1810-10039,
archivePrefix = {arXiv},
arxivId = {1810.10039},
author = {Bobrow, Taylor L and Mahmood, Faisal and Inserni, Miguel and Durr, Nicholas J and Urr, N Icholas J D},
eprint = {1810.10039},
file = {:Users/sergio/Documents/TFG/related papers/1810.10039.pdf:pdf},
journal = {CoRR},
title = {{DeepLSR: Deep learning approach for laser speckle reduction}},
url = {http://arxiv.org/abs/1810.10039},
volume = {abs/1810.1},
year = {2018}
}
@article{Gareau2009,
abstract = {Fluorescence confocal mosaicing microscopy of tissue biopsies stained with acridine orange has been shown to accurately identify tumors and with an overall sensitivity of 96.6{\%} and specificity of 89.2{\%}. However, fluorescence shows only nuclear detail similar to hematoxylin in histopathology and does not show collagen or cytoplasm, which may provide necessary negative contrast information similar to eosin used in histopathology. Reflectance mode contrast is sensitive to collagen and cytoplasm without staining. To further improve sensitivity and specificity, digitally stained confocal mosaics combine confocal fluorescence and reflectance images in a multimodal pseudo-color image to mimic the appearance of histopathology with hematoxylin and eosin and facilitate the introduction of confocal microscopy into the clinical realm.},
author = {Gareau, Daniel S.},
doi = {10.1117/1.3149853},
file = {:Users/sergio/Documents/TFG/related papers/nihms227243.pdf:pdf},
issn = {1083-3668},
journal = {Journal of biomedical optics},
keywords = {Basal Cell/pathology,Carcinoma,Confocal/*methods,Eosine Yellowish-(YS)/chemistry,Fluorescence/*methods,Hematoxylin/chemistry,Histocytochemistry/*methods,Histocytological Preparation Techniques/*methods,Humans,Microscopy,Sensitivity and Specificity,Skin Neoplasms/pathology,Skin/cytology},
language = {eng},
number = {3},
pages = {34050},
title = {{Feasibility of digitally stained multimodal confocal mosaics to simulate histopathology}},
url = {https://www.ncbi.nlm.nih.gov/pubmed/19566342 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2929174/},
volume = {14},
year = {2009}
}
@inproceedings{Ronneberger2015,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-24574-4_28},
eprint = {1505.04597},
file = {:Users/sergio/Documents/TFG/related papers/1505.04597.pdf:pdf},
isbn = {9783319245737},
issn = {16113349},
pmid = {23285570},
title = {{U-net: Convolutional networks for biomedical image segmentation}},
year = {2015}
}
@article{Mirza2018,
abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
archivePrefix = {arXiv},
arxivId = {1411.1784},
author = {Mirza, Mehdi and Osindero, Simon},
eprint = {1411.1784},
file = {:Users/sergio/Documents/TFG/related papers/1411.1784.pdf:pdf},
journal = {arXiv:1411.1784v1 [cs.LG] 6 Nov 2014 Conditional},
month = {nov},
title = {{Conditional Generative Adversarial Nets}},
url = {http://arxiv.org/abs/1411.1784},
year = {2014}
}
@inproceedings{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {arXiv:1311.2901v3},
author = {Zeiler, Matthew D and Fergus, Rob},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {arXiv:1311.2901v3},
file = {:Users/sergio/Documents/TFG/related papers/1311.2901.pdf:pdf},
isbn = {9783319105895},
issn = {16113349},
number = {PART 1},
pages = {818--833},
title = {{Visualizing and understanding convolutional networks}},
volume = {8689 LNCS},
year = {2014}
}
@article{Zhang2012,
abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
archivePrefix = {arXiv},
arxivId = {1705.07874},
author = {Lundberg, Scott and Lee, Su-In},
eprint = {1705.07874},
file = {:Users/sergio/Documents/7062-a-unified-approach-to-interpreting-model-predictions.pdf:pdf},
issn = {10495258},
number = {3},
pages = {426--430},
title = {{A Unified Approach to Interpreting Model Predictions}},
url = {http://arxiv.org/abs/1705.07874},
volume = {16},
year = {2017}
}
@inproceedings{Lin2018,
abstract = {Image-to-image translation tasks have been widely investigated with Generative Adversarial Networks (GANs) and dual learning. However, existing models lack the ability to control the translated results in the target domain and their results usually lack of diversity in the sense that a fixed image usually leads to (almost) deterministic translation result. In this paper, we study a new problem, conditional image-to-image translation, which is to translate an image from the source domain to the target domain conditioned on a given image in the target domain. It requires that the generated image should inherit some domain-specific features of the conditional image from the target domain. Therefore, changing the conditional image in the target domain will lead to diverse translation results for a fixed input image from the source domain, and therefore the conditional input image helps to control the translation results. We tackle this problem with unpaired data based on GANs and dual learning. We twist two conditional translation models (one translation from A domain to B domain, and the other one from B domain to A domain) together for inputs combination and reconstruction while preserving domain independent features. We carry out experiments on men's faces from-to women's faces translation and edges to shoes{\&}bags translations. The results demonstrate the effectiveness of our proposed method.},
author = {Lin, Jianxin and Xia, Yingce and Qin, Tao and Chen, Zhibo and Liu, Tie Yan},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00579},
isbn = {9781538664209},
issn = {10636919},
pages = {5524--5532},
title = {{Conditional Image-to-Image Translation}},
year = {2018}
}
@article{Zhu2017,
abstract = {Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a $\backslash$emph{\{}distribution{\}} of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.},
archivePrefix = {arXiv},
arxivId = {1711.11586},
author = {Zhu, Jun-Yan and Zhang, Richard and Pathak, Deepak and Darrell, Trevor and Efros, Alexei A. and Wang, Oliver and Shechtman, Eli},
eprint = {1711.11586},
file = {:Users/sergio/Library/Application Support/Mendeley Desktop/Downloaded/Zhu et al. - 2017 - Toward Multimodal Image-to-Image Translation.pdf:pdf},
month = {nov},
title = {{Toward Multimodal Image-to-Image Translation}},
url = {http://arxiv.org/abs/1711.11586},
year = {2017}
}
@article{Kim2017,
archivePrefix = {arXiv},
arxivId = {1703.05192},
author = {Kim, Taeksoo and Cha, Moonsu and Kim, Hyunsoo and Lee, Jung Kwon and Kim, Jiwon},
eprint = {1703.05192},
file = {:Users/sergio/Library/Application Support/Mendeley Desktop/Downloaded/Kim et al. - 2017 - Learning to Discover Cross-Domain Relations with Generative Adversarial Networks.pdf:pdf},
month = {mar},
title = {{Learning to Discover Cross-Domain Relations with Generative Adversarial Networks}},
url = {https://arxiv.org/abs/1703.05192},
year = {2017}
}
@article{Gareau2009a,
abstract = {Fluorescence confocal mosaicing microscopy of tissue biopsies stained with acridine orange has been shown to accurately identify tumors and with an overall sensitivity of 96.6{\%} and specificity of 89.2{\%}. However, fluorescence shows only nuclear detail similar to hematoxylin in histopathology and does not show collagen or cytoplasm, which may provide necessary negative contrast information similar to eosin used in histopathology. Reflectance mode contrast is sensitive to collagen and cytoplasm without staining. To further improve sensitivity and specificity, digitally stained confocal mosaics combine confocal fluorescence and reflectance images in a multimodal pseudo-color image to mimic the appearance of histopathology with hematoxylin and eosin and facilitate the introduction of confocal microscopy into the clinical realm.},
author = {Gareau, Daniel S.},
doi = {10.1117/1.3149853},
file = {:Users/sergio/Documents/TFG/related papers/nihms227243.pdf:pdf},
issn = {10833668},
journal = {Journal of Biomedical Optics},
number = {3},
pages = {034050},
title = {{Feasibility of digitally stained multimodal confocal mosaics to simulate histopathology}},
volume = {14},
year = {2009}
}
@article{Tellez2018,
abstract = {IEEE Manual counting of mitotic tumor cells in tissue sections constitutes one of the strongest prognostic markers for breast cancer. This procedure, however, is time-consuming and error-prone. We developed a method to automatically detect mitotic figures in breast cancer tissue sections based on convolutional neural networks (CNNs). Application of CNNs to hematoxylin and eosin (H {\&} {\#}x0026;E) stained histological tissue sections is hampered by: (1) noisy and expensive reference standards established by pathologists, (2) lack of generalization due to staining variation across laboratories, and (3) high computational requirements needed to process gigapixel whole-slide images (WSIs). In this paper, we present a method to train and evaluate CNNs to specifically solve these issues in the context of mitosis detection in breast cancer WSIs. First, by combining image analysis of mitotic activity in phosphohistone-H3 (PHH3) restained slides and registration, we built a reference standard for mitosis detection in entire H {\&} {\#}x0026;E WSIs requiring minimal manual annotation effort. Second, we designed a data augmentation strategy that creates diverse and realistic H {\&} {\#}x0026;E stain variations by modifying the hematoxylin and eosin color channels directly. Using it during training combined with network ensembling resulted in a stain invariant mitosis detector. Third, we applied knowledge distillation to reduce the computational requirements of the mitosis detection ensemble with a negligible loss of performance. The system was trained in a single-center cohort and evaluated in an independent multicenter cohort from The Cancer Genome Atlas on the three tasks of the Tumor Proliferation Assessment Challenge (TUPAC). We obtained a performance within the top-3 best methods for most of the tasks of the challenge.},
author = {Tellez, David and Balkenhol, Maschenka and Otte-H{\"{o}}ller, Irene and {Van De Loo}, Rob and Vogels, Rob and Bult, Peter and Wauters, Carla and Vreuls, Willem and Mol, Suzanne and Karssemeijer, Nico and Litjens, Geert and {Van Der Laak}, Jeroen and Ciompi, Francesco},
doi = {10.1109/TMI.2018.2820199},
file = {:Users/sergio/Documents/TFG/related papers/08327641.pdf:pdf},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Breast cancer,convolutional neural networks,data augmentation,knowledge distillation,mitosis detection,phosphohistone-H3},
title = {{Whole-Slide Mitosis Detection in H{\&}E Breast Histology Using PHH3 as a Reference to Train Distilled Stain-Invariant Convolutional Networks}},
year = {2018}
}
@article{Radford2015,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
eprint = {1511.06434},
file = {:Users/sergio/Library/Application Support/Mendeley Desktop/Downloaded/Radford, Metz, Chintala - 2015 - Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.pdf:pdf},
month = {nov},
title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1511.06434},
year = {2015}
}
@article{Yu2015,
abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
archivePrefix = {arXiv},
arxivId = {1511.07122},
author = {Yu, Fisher and Koltun, Vladlen},
eprint = {1511.07122},
file = {:Users/sergio/Documents/TFG/related papers/1511.07122.pdf:pdf},
title = {{Multi-Scale Context Aggregation by Dilated Convolutions}},
url = {http://arxiv.org/abs/1511.07122},
year = {2015}
}
@article{DBLP:journals/corr/SalimansGZCRC16,
archivePrefix = {arXiv},
arxivId = {1606.03498},
author = {Salimans, Tim and Goodfellow, Ian J and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
eprint = {1606.03498},
file = {:Users/sergio/Documents/TFG/related papers/1606.03498.pdf:pdf},
journal = {CoRR},
title = {{Improved Techniques for Training GANs}},
url = {http://arxiv.org/abs/1606.03498},
volume = {abs/1606.0},
year = {2016}
}
@article{rs10020196,
abstract = {In this paper, to break the limit of the traditional linear models for synthetic aperture radar (SAR) image despeckling, we propose a novel deep learning approach by learning a non-linear end-to-end mapping between the noisy and clean SAR images with a dilated residual network (SAR-DRN). SAR-DRN is based on dilated convolutions, which can both enlarge the receptive field and maintain the filter size and layer depth with a lightweight structure. In addition, skip connections and a residual learning strategy are added to the despeckling model to maintain the image details and reduce the vanishing gradient problem. Compared with the traditional despeckling methods, the proposed method shows a superior performance over the state-of-the-art methods in both quantitative and visual assessments, especially for strong speckle noise.},
author = {Zhang, Qiang and Yuan, Qiangqiang and Li, Jie and Yang, Zhen and Ma, Xiaoshuang},
doi = {10.3390/rs10020196},
file = {:Users/sergio/Documents/TFG/related papers/remotesensing-10-00196.pdf:pdf},
issn = {2072-4292},
journal = {Remote Sensing},
number = {2},
title = {{Learning a Dilated Residual Network for SAR Image Despeckling}},
url = {https://www.mdpi.com/2072-4292/10/2/196},
volume = {10},
year = {2018}
}
@article{Ma2018,
abstract = {Unsupervised image translation, which aims in translating two independent sets of images, is challenging in discovering the correct correspondences without paired data. Existing works build upon Generative Adversarial Network (GAN) such that the distribution of the translated images are indistinguishable from the distribution of the target set. However, such set-level constraints cannot learn the instance-level correspondences (e.g. aligned semantic parts in object configuration task). This limitation often results in false positives (e.g. geometric or semantic artifacts), and further leads to mode collapse problem. To address the above issues, we propose a novel framework for instance-level image translation by Deep Attention GAN (DA-GAN). Such a design enables DA-GAN to decompose the task of translating samples from two sets into translating instances in a highly-structured latent space. Specifically, we jointly learn a deep attention encoder, and the instancelevel correspondences could be consequently discovered through attending on the learned instance pairs. Therefore, the constraints could be exploited on both set-level and instance-level. Comparisons against several state-ofthe- arts demonstrate the superiority of our approach, and the broad application capability, e.g, pose morphing, data augmentation, etc., pushes the margin of domain translation problem.},
author = {Ma, Shuang and Fu, Jianlong and Chen, Chang Wen and Mei, Tao},
doi = {10.1109/CVPR.2018.00593},
file = {:Users/sergio/Documents/TFG/related papers/DA-GAN.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {5657--5666},
publisher = {IEEE},
title = {{DA-GAN: Instance-Level Image Translation by Deep Attention Generative Adversarial Networks}},
year = {2018}
}
@inproceedings{Han2017,
abstract = {Phase Contrast (PC) and Differential Interference Contrast (DIC) microscopy are two popular non-invasive techniques for monitoring live cells. Each of these two image modalities has its own advantages and disadvantages to visualize specimens, so biologists need these two complementary modalities together to analyze specimens. In this paper, we investigate a conditional Generative Adversarial Network (conditional GAN), which contains one generator and two discriminators, to transfer microscopy image modalities. Given a training dataset consisting of pairs of images (source and destination) captured on the same set of specimens by DIC and Phase Contrast microscopes, we can train a conditional GAN, and with this well-trained GAN, we can generate the corresponding Phase Contrast image given a new DIC image, vice versa. The preliminary experiments demonstrate that our approach outperforms one state-of-the-arts method, and can provide biologists a computational way to switch between microscopy image modalities, so biologists can combine the advantages of different image modalities to better visualize and analyze specimens over time, without purchasing all types of microscopy image modalities or switching between imaging systems back-andforth during time-lapse experiments.},
author = {Han, Liang and Yin, Zhaozheng},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2017.118},
file = {:Users/sergio/Documents/TFG/related papers/08014852.pdf:pdf},
isbn = {9781538607336},
issn = {21607516},
pages = {851--859},
title = {{Transferring Microscopy Image Modalities with Conditional Generative Adversarial Networks}},
volume = {2017-July},
year = {2017}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2661v1},
author = {Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
doi = {10.1016/B978-0-408-00109-0.50001-8},
eprint = {arXiv:1406.2661v1},
file = {:Users/sergio/Documents/TFG/related papers/1406.2661.pdf:pdf},
isbn = {1406.2661},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 27},
pages = {1--9},
pmid = {15645445},
title = {{Generative Adversarial Nets}},
url = {https://arxiv.org/abs/1406.2661},
year = {2014}
}
@techreport{Bel2019,
abstract = {The performance of deep learning applications in digital histopathology can deteriorate significantly due to staining variations across centers. We employ cycle-consistent generative adversarial networks (cycleGANs) for unpaired image-to-image translation, facilitating between-center stain transformation. We find that modifications to the original cycleGAN architecture make it more suitable for stain transformation, creating artificially stained images of high quality. Specifically, changing the generator model to a smaller U-net-like architecture, adding an identity loss term, increasing the batch size and the learning all led to improved training stability and performance. Furthermore, we propose a method for dealing with tiling artifacts when applying the network on whole slide images (WSIs). We apply our stain transformation method on two datasets of PAS-stained (Periodic Acid-Schiff) renal tissue sections from different centers. We show that stain transformation is beneficial to the performance of cross-center segmentation, raising the Dice coefficient from 0.36 to 0.85 and from 0.45 to 0.73 on the two datasets.},
author = {de Bel, Thomas and Hermsen, Meyke and {Jesper Kers}, Rumcnl and van der Laak, Jeroen and Litjens, Geert},
booktitle = {Proceedings of Machine Learning Research},
file = {:Users/sergio/Documents/TFG/related papers/Stain-transforming cycle-consistent generative adversarial networks for improved segmentation of renal histopathology.pdf:pdf},
keywords = {Deep learning,generative adversarial networks,medical imaging,stain transformation},
pages = {151--163},
title = {{Stain-Transforming Cycle-Consistent Generative Adversarial Networks for Improved Segmentation of Renal Histopathology}},
url = {http://proceedings.mlr.press/v102/de-bel19a/de-bel19a.pdf},
volume = {102},
year = {2019}
}
@article{Zhu2017a,
abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain {\$}X{\$} to a target domain {\$}Y{\$} in the absence of paired examples. Our goal is to learn a mapping {\$}G: X \backslashrightarrow Y{\$} such that the distribution of images from {\$}G(X){\$} is indistinguishable from the distribution {\$}Y{\$} using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping {\$}F: Y \backslashrightarrow X{\$} and introduce a cycle consistency loss to push {\$}F(G(X)) \backslashapprox X{\$} (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
archivePrefix = {arXiv},
arxivId = {1703.10593},
author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
eprint = {1703.10593},
file = {:Users/sergio/Library/Application Support/Mendeley Desktop/Downloaded/Zhu et al. - 2017 - Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.pdf:pdf},
month = {mar},
title = {{Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks}},
url = {http://arxiv.org/abs/1703.10593},
year = {2017}
}
@article{DBLP:journals/corr/Goodfellow17,
archivePrefix = {arXiv},
arxivId = {1701.00160},
author = {Goodfellow, Ian J},
eprint = {1701.00160},
file = {:Users/sergio/Documents/TFG/related papers/1701.00160.pdf:pdf},
journal = {CoRR},
title = {{{\{}NIPS{\}} 2016 Tutorial: Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1701.00160},
volume = {abs/1701.0},
year = {2017}
}
@article{Lu2017,
abstract = {We are interested in attribute-guided face generation: given a low-res face input image, an attribute vector that can be extracted from a high-res image (attribute image), our new method generates a high-res face image for the low-res input that satisfies the given attributes. To address this problem, we condition the CycleGAN and propose conditional CycleGAN, which is designed to 1) handle unpaired training data because the training low/high-res and high-res attribute images may not necessarily align with each other, and to 2) allow easy control of the appearance of the generated face via the input attributes. We demonstrate impressive results on the attribute-guided conditional CycleGAN, which can synthesize realistic face images with appearance easily controlled by user-supplied attributes (e.g., gender, makeup, hair color, eyeglasses). Using the attribute image as identity to produce the corresponding conditional vector and by incorporating a face verification network, the attribute-guided network becomes the identity-guided conditional CycleGAN which produces impressive and interesting results on identity transfer. We demonstrate three applications on identity-guided conditional CycleGAN: identity-preserving face superresolution, face swapping, and frontal face generation, which consistently show the advantage of our new method.},
archivePrefix = {arXiv},
arxivId = {1705.09966},
author = {Lu, Yongyi and Tai, Yu-Wing and Tang, Chi-Keung},
eprint = {1705.09966},
file = {:Users/sergio/Library/Application Support/Mendeley Desktop/Downloaded/Lu, Tai, Tang - 2017 - Attribute-Guided Face Generation Using Conditional CycleGAN.pdf:pdf},
month = {may},
title = {{Attribute-Guided Face Generation Using Conditional CycleGAN}},
url = {http://arxiv.org/abs/1705.09966},
year = {2017}
}
@article{Vaswani2017,
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
eprint = {1706.03762},
file = {:Users/sergio/Library/Application Support/Mendeley Desktop/Downloaded/Vaswani et al. - 2017 - Attention Is All You Need(2).pdf:pdf},
month = {jun},
title = {{Attention Is All You Need}},
url = {https://arxiv.org/abs/1706.03762},
year = {2017}
}
@inproceedings{8313133,
abstract = {Synthetic Aperture Radar (SAR) images are often contaminated by a multiplicative noise known as speckle. Speckle makes the processing and interpretation of SAR images difficult. We propose a deep learning-based approach called, Image Despeckling Generative Adversarial Network (ID-GAN), for automatically removing speckle from the input noisy images. In particular, ID-GAN is trained in an end-to-end fashion using a combination of Euclidean loss, Perceptual loss and Adversarial loss. Extensive experiments on synthetic and real SAR images show that the proposed method achieves significant improvements over the state-of-the-art speckle reduction methods.},
author = {Wang, P and Zhang, H and Patel, V M},
booktitle = {2017 IEEE 7th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)},
doi = {10.1109/CAMSAP.2017.8313133},
file = {:Users/sergio/Documents/TFG/related papers/08313133.pdf:pdf},
keywords = {image denoising;image restoration;learning (artifi},
month = {dec},
pages = {1--5},
title = {{Generative adversarial network-based restoration of speckled SAR images}},
year = {2017}
}
@article{Arjovsky2017,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
archivePrefix = {arXiv},
arxivId = {1701.07875},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
eprint = {1701.07875},
file = {:Users/sergio/Documents/TFG/related papers/1701.07875.pdf:pdf},
title = {{Wasserstein GAN}},
url = {http://arxiv.org/abs/1701.07875},
year = {2017}
}
@article{DBLP:journals/corr/ChierchiaCPV17,
archivePrefix = {arXiv},
arxivId = {1704.00275},
author = {Chierchia, Giovanni and Cozzolino, Davide and Poggi, Giovanni and Verdoliva, Luisa and France, F- Noisy-le-grand},
eprint = {1704.00275},
file = {:Users/sergio/Documents/TFG/related papers/1704.00275.pdf:pdf},
journal = {CoRR},
title = {{SAR image despeckling through convolutional neural networks}},
url = {http://arxiv.org/abs/1704.00275},
volume = {abs/1704.0},
year = {2017}
}
@inproceedings{Krizhevsky:2012:ICD:2999134.2999257,
address = {USA},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
file = {:Users/sergio/Documents/TFG/related papers/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf:pdf},
pages = {1097--1105},
publisher = {Curran Associates Inc.},
series = {NIPS'12},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://dl.acm.org/citation.cfm?id=2999134.2999257},
year = {2012}
}
@inproceedings{miyato2018spectral,
author = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi and Networks, Preferred},
booktitle = {International Conference on Learning Representations},
file = {:Users/sergio/Documents/TFG/related papers/e7852ae61dc5192febbc9f41621787f33116f09d.pdf:pdf},
title = {{Spectral Normalization for Generative Adversarial Networks}},
url = {https://openreview.net/forum?id=B1QRgziT-},
year = {2018}
}
@inproceedings{Gigilashvili2018,
abstract = {Speckle noise is a form of multiplicative noise that corrupts the quality of medical images. It is well described and studied in medical ultrasound imaging, but less attention has been paid to its presence in reflectance microscopy images. Presence of the speckle noise not only limits the application of further post-processing and computer vision techniques, like edge detection, but it also makes diagnostics more difficult and less reliable for physicians. Many speckle mitigation techniques have been studied by various researchers, but the vast majority of them limit themselves to a single image of the target tissue. In this study, we examined the possibility to mitigate speckle using the redundant data present in consecutive frames of the video recordings, as they provide uncorrelated data from different spatial positions. Different ways of processing the redundant data were examined and compared against conventional methodologies.},
author = {Gigilashvili, D and Yin, Chengbo and Liu, J T C and Hardeberg, J Y and Pedersen, M},
booktitle = {2018 14th International Conference on Signal-Image Technology {\&} Internet-Based Systems (SITIS)},
doi = {10.1109/SITIS.2018.00050},
file = {:Users/sergio/Documents/TFG/related papers/08706173.pdf:pdf},
isbn = {VO  -},
keywords = {Biomedical imaging,Confocal microscopy,Denoising,Despeckling,Filtering,Image processing,Indexes,Medical imaging,Microscopy,Speckle,Three-dimensional displays,Two dimensional displays,biological tissues,biomedical optical imaging,biomedical ultrasonics,computer vision,computer vision techniques,dual-axis confocal microscopy,edge detection,image denoising,medical image processing,medical images,medical ultrasound imaging,optical microscopy,reflectance microscopy images,speckle,speckle mitigation techniques,speckle noise},
pages = {281--288},
publisher = {IEEE},
title = {{Measuring and Mitigating Speckle Noise in Dual-Axis Confocal Microscopy Images}},
year = {2018}
}
@article{Lin2019,
abstract = {Humans can only interact with part of the surrounding environment due to biological restrictions. Therefore, we learn to reason the spatial relationships across a series of observations to piece together the surrounding environment. Inspired by such behavior and the fact that machines also have computational constraints, we propose $\backslash$underline{\{}CO{\}}nditional $\backslash$underline{\{}CO{\}}ordinate GAN (COCO-GAN) of which the generator generates images by parts based on their spatial coordinates as the condition. On the other hand, the discriminator learns to justify realism across multiple assembled patches by global coherence, local appearance, and edge-crossing continuity. Despite the full images are never generated during training, we show that COCO-GAN can produce $\backslash$textbf{\{}state-of-the-art-quality{\}} full images during inference. We further demonstrate a variety of novel applications enabled by teaching the network to be aware of coordinates. First, we perform extrapolation to the learned coordinate manifold and generate off-the-boundary patches. Combining with the originally generated full image, COCO-GAN can produce images that are larger than training samples, which we called "beyond-boundary generation". We then showcase panorama generation within a cylindrical coordinate system that inherently preserves horizontally cyclic topology. On the computation side, COCO-GAN has a built-in divide-and-conquer paradigm that reduces memory requisition during training and inference, provides high-parallelism, and can generate parts of images on-demand.},
archivePrefix = {arXiv},
arxivId = {1904.00284},
author = {Lin, Chieh Hubert and Chang, Chia-Che and Chen, Yu-Sheng and Juan, Da-Cheng and Wei, Wei and Chen, Hwann-Tzong},
eprint = {1904.00284},
file = {:Users/sergio/Documents/TFG/related papers/1904.00284.pdf:pdf},
title = {{COCO-GAN: Generation by Parts via Conditional Coordinating}},
url = {http://arxiv.org/abs/1904.00284},
year = {2019}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2661v1},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
doi = {10.1016/B978-0-408-00109-0.50001-8},
eprint = {arXiv:1406.2661v1},
isbn = {1406.2661},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 27},
pmid = {15645445},
title = {{Generative Adversarial Nets (NIPS version)}},
url = {https://arxiv.org/abs/1406.2661},
year = {2014}
}
@inproceedings{Wang2018,
abstract = {We propose a novel approach for generating high quality visible-like images from Synthetic Aperture Radar (SAR) images using Deep Convolutional Generative Adversarial Network (GAN) architectures. The proposed approach is based on a cascaded network of convolutional neural nets (CNNs) for despeckling and image colorization. The cascaded structure results in faster convergence during training and produces high quality visible images from the corresponding SAR images. Experimental results on both simulated and real SAR images show that the proposed method can produce visible-like images better compared to the recent state-of-the-art deep learning-based methods.},
author = {Wang, Puyang and Patel, Vishal M.},
booktitle = {2018 IEEE Radar Conference, RadarConf 2018},
doi = {10.1109/RADAR.2018.8378622},
file = {:Users/sergio/Documents/TFG/related papers/Generating High Quality Visible Images from SAR Images Using CNNs.pdf:pdf;:Users/sergio/Documents/TFG/related papers/wang2018.pdf:pdf},
isbn = {9781538641675},
keywords = {Synthetic aperture radar image,colorization,despeckling},
pages = {570--575},
title = {{Generating high quality visible images from SAR images using CNNs}},
year = {2018}
}
@article{Zakharov2019,
abstract = {Several recent works have shown how highly realistic human head images can be obtained by training convolutional neural networks to generate them. In order to create a personalized talking head model, these works require training on a large dataset of images of a single person. However, in many practical scenarios, such personalized talking head models need to be learned from a few image views of a person, potentially even a single image. Here, we present a system with such few-shot capability. It performs lengthy meta-learning on a large dataset of videos, and after that is able to frame few- and one-shot learning of neural talking head models of previously unseen people as adversarial training problems with high capacity generators and discriminators. Crucially, the system is able to initialize the parameters of both the generator and the discriminator in a person-specific way, so that training can be based on just a few images and done quickly, despite the need to tune tens of millions of parameters. We show that such an approach is able to learn highly realistic and personalized talking head models of new people and even portrait paintings.},
archivePrefix = {arXiv},
arxivId = {1905.08233},
author = {Zakharov, Egor and Shysheya, Aliaksandra and Burkov, Egor and Lempitsky, Victor},
eprint = {1905.08233},
file = {:Users/sergio/Documents/1905.08233.pdf:pdf},
title = {{Few-Shot Adversarial Learning of Realistic Neural Talking Head Models}},
url = {http://arxiv.org/abs/1905.08233},
year = {2019}
}
@misc{Hinton2012RMSProp,
author = {Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
title = {Overview of mini-batch gradient descent},
year = {2012 (accessed September 14, 2019)},
howpublished = "\url{https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}"
}
@misc{Kingma2014Adam,
title={Adam: A Method for Stochastic Optimization},
author={Diederik P. Kingma and Jimmy Ba},
year={2014},
eprint={1412.6980},
archivePrefix={arXiv},
primaryClass={cs.LG}
}
@misc{Zeiler2012ADADELTA,
title={ADADELTA: An Adaptive Learning Rate Method},
author={Matthew D. Zeiler},
year={2012},
eprint={1212.5701},
archivePrefix={arXiv},
primaryClass={cs.LG}
}
@misc{Adagrad,
title={Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
author={Duchi, John and Hazan, Elad and Singer, Yoram},
year={2011},
journal={Journal of Machine Learning Research (JMLR)}
}
