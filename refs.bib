%% Related papers.
@article{Combalia2019,
abstract = {Specialists have used confocal microscopy in the ex-vivo modality to identify tumors with an overall sensitivity of 96.6{\%} and speciﬁcity of 89.2{\%}. However, this technology hasn't established yet in the standard clinical practice because most pathologists lack the knowledge to interpret its output. In this paper we propose a combination of deep learning and computer vision techniques to digitally stain confocal microscopy images into H{\&}E-like slides, enabling pathologists to interpret these images without speciﬁc training. We use a fully convolutional neural network with a multiplicative residual connection to denoise the confocal microscopy images, and then stain them using a Cycle Consistency Generative Adversarial Network.},
author = {Combalia, Marc and P{\'{e}}rez-Anker, Javiera and Garc{\'{i}}a-Herrera, Adriana and Alos, Ll{\'{u}}cia and Vilaplana, Ver{\'{o}}nica and Marqu{\'{e}}s, Ferra and Puig, Susana and Malvehy, Josep},
file = {:Users/sergio/Documents/TFG/related papers/Digitally Stained Confocal Microscopy through Deep Learning.pdf:pdf;:Users/sergio/Library/Application Support/Mendeley Desktop/Downloaded/Combalia et al. - 2019 - Digitally Stained Confocal Microscopy through Deep Learning.pdf:pdf},
journal = {Proceedings of Machine Learning Research},
keywords = {Confocal Microscopy,CycleGAN,Deep learning,Digital Staining,Neural Networks,Speckle Noise},
pages = {1--9},
title = {{Digitally Stained Confocal Microscopy through Deep Learning}},
year = {2019}
}
@article{Gareau2009,
abstract = {Fluorescence confocal mosaicing microscopy of tissue biopsies stained with acridine orange has been shown to accurately identify tumors and with an overall sensitivity of 96.6{\%} and specificity of 89.2{\%}. However, fluorescence shows only nuclear detail similar to hematoxylin in histopathology and does not show collagen or cytoplasm, which may provide necessary negative contrast information similar to eosin used in histopathology. Reflectance mode contrast is sensitive to collagen and cytoplasm without staining. To further improve sensitivity and specificity, digitally stained confocal mosaics combine confocal fluorescence and reflectance images in a multimodal pseudo-color image to mimic the appearance of histopathology with hematoxylin and eosin and facilitate the introduction of confocal microscopy into the clinical realm.},
author = {Gareau, Daniel S.},
doi = {10.1117/1.3149853},
file = {:Users/sergio/Documents/TFG/related papers/nihms227243.pdf:pdf},
issn = {1083-3668},
journal = {Journal of biomedical optics},
keywords = {Basal Cell/pathology,Carcinoma,Confocal/*methods,Eosine Yellowish-(YS)/chemistry,Fluorescence/*methods,Hematoxylin/chemistry,Histocytochemistry/*methods,Histocytological Preparation Techniques/*methods,Humans,Microscopy,Sensitivity and Specificity,Skin Neoplasms/pathology,Skin/cytology},
language = {eng},
number = {3},
pages = {34050},
title = {{Feasibility of digitally stained multimodal confocal mosaics to simulate histopathology}},
url = {https://www.ncbi.nlm.nih.gov/pubmed/19566342 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2929174/},
volume = {14},
year = {2009}
}
@techreport{Bel2019,
abstract = {The performance of deep learning applications in digital histopathology can deteriorate significantly due to staining variations across centers. We employ cycle-consistent generative adversarial networks (cycleGANs) for unpaired image-to-image translation, facilitating between-center stain transformation. We find that modifications to the original cycleGAN architecture make it more suitable for stain transformation, creating artificially stained images of high quality. Specifically, changing the generator model to a smaller U-net-like architecture, adding an identity loss term, increasing the batch size and the learning all led to improved training stability and performance. Furthermore, we propose a method for dealing with tiling artifacts when applying the network on whole slide images (WSIs). We apply our stain transformation method on two datasets of PAS-stained (Periodic Acid-Schiff) renal tissue sections from different centers. We show that stain transformation is beneficial to the performance of cross-center segmentation, raising the Dice coefficient from 0.36 to 0.85 and from 0.45 to 0.73 on the two datasets.},
author = {de Bel, Thomas and Hermsen, Meyke and {Jesper Kers}, Rumcnl and van der Laak, Jeroen and Litjens, Geert},
booktitle = {Proceedings of Machine Learning Research},
file = {:Users/sergio/Documents/TFG/related papers/Stain-transforming cycle-consistent generative adversarial networks for improved segmentation of renal histopathology.pdf:pdf},
keywords = {Deep learning,generative adversarial networks,medical imaging,stain transformation},
pages = {151--163},
title = {{Stain-Transforming Cycle-Consistent Generative Adversarial Networks for Improved Segmentation of Renal Histopathology}},
url = {http://proceedings.mlr.press/v102/de-bel19a/de-bel19a.pdf},
volume = {102},
year = {2019}
}
@article{Shaban2018,
archivePrefix = {arXiv},
arxivId = {1804.01601},
author = {Shaban, M Tarek and Baur, Christoph and Navab, Nassir and Albarqouni, Shadi},
eprint = {1804.01601},
file = {:Users/sergio/Library/Application Support/Mendeley Desktop/Downloaded/Shaban et al. - 2018 - StainGAN Stain Style Transfer for Digital Histological Images.pdf:pdf},
month = {apr},
title = {{StainGAN: Stain Style Transfer for Digital Histological Images}},
url = {https://arxiv.org/abs/1804.01601},
year = {2018}
}
@inproceedings{Wang2018,
abstract = {We propose a novel approach for generating high quality visible-like images from Synthetic Aperture Radar (SAR) images using Deep Convolutional Generative Adversarial Network (GAN) architectures. The proposed approach is based on a cascaded network of convolutional neural nets (CNNs) for despeckling and image colorization. The cascaded structure results in faster convergence during training and produces high quality visible images from the corresponding SAR images. Experimental results on both simulated and real SAR images show that the proposed method can produce visible-like images better compared to the recent state-of-the-art deep learning-based methods.},
author = {Wang, Puyang and Patel, Vishal M.},
booktitle = {2018 IEEE Radar Conference, RadarConf 2018},
doi = {10.1109/RADAR.2018.8378622},
file = {:Users/sergio/Documents/TFG/related papers/Generating High Quality Visible Images from SAR Images Using CNNs.pdf:pdf},
isbn = {9781538641675},
keywords = {Synthetic aperture radar image,colorization,despeckling},
pages = {570--575},
title = {{Generating high quality visible images from SAR images using CNNs}},
year = {2018}
}
@article{DBLP:journals/corr/abs-1810-10039,
archivePrefix = {arXiv},
arxivId = {1810.10039},
author = {Bobrow, Taylor L and Mahmood, Faisal and Inserni, Miguel and Durr, Nicholas J and Urr, N Icholas J D},
eprint = {1810.10039},
file = {:Users/sergio/Documents/TFG/related papers/1810.10039.pdf:pdf},
journal = {CoRR},
title = {{DeepLSR: Deep learning approach for laser speckle reduction}},
url = {http://arxiv.org/abs/1810.10039},
volume = {abs/1810.1},
year = {2018}
}
@article{rs10020196,
abstract = {In this paper, to break the limit of the traditional linear models for synthetic aperture radar (SAR) image despeckling, we propose a novel deep learning approach by learning a non-linear end-to-end mapping between the noisy and clean SAR images with a dilated residual network (SAR-DRN). SAR-DRN is based on dilated convolutions, which can both enlarge the receptive field and maintain the filter size and layer depth with a lightweight structure. In addition, skip connections and a residual learning strategy are added to the despeckling model to maintain the image details and reduce the vanishing gradient problem. Compared with the traditional despeckling methods, the proposed method shows a superior performance over the state-of-the-art methods in both quantitative and visual assessments, especially for strong speckle noise.},
author = {Zhang, Qiang and Yuan, Qiangqiang and Li, Jie and Yang, Zhen and Ma, Xiaoshuang},
doi = {10.3390/rs10020196},
file = {:Users/sergio/Documents/TFG/related papers/remotesensing-10-00196.pdf:pdf},
issn = {2072-4292},
journal = {Remote Sensing},
number = {2},
title = {{Learning a Dilated Residual Network for SAR Image Despeckling}},
url = {https://www.mdpi.com/2072-4292/10/2/196},
volume = {10},
year = {2018}
}
@article{DBLP:journals/corr/ChierchiaCPV17,
archivePrefix = {arXiv},
arxivId = {1704.00275},
author = {Chierchia, Giovanni and Cozzolino, Davide and Poggi, Giovanni and Verdoliva, Luisa and France, F- Noisy-le-grand},
eprint = {1704.00275},
file = {:Users/sergio/Documents/TFG/related papers/1704.00275.pdf:pdf},
journal = {CoRR},
title = {{SAR image despeckling through convolutional neural networks}},
url = {http://arxiv.org/abs/1704.00275},
volume = {abs/1704.0},
year = {2017}
}
@inproceedings{8313133,
abstract = {Synthetic Aperture Radar (SAR) images are often contaminated by a multiplicative noise known as speckle. Speckle makes the processing and interpretation of SAR images difficult. We propose a deep learning-based approach called, Image Despeckling Generative Adversarial Network (ID-GAN), for automatically removing speckle from the input noisy images. In particular, ID-GAN is trained in an end-to-end fashion using a combination of Euclidean loss, Perceptual loss and Adversarial loss. Extensive experiments on synthetic and real SAR images show that the proposed method achieves significant improvements over the state-of-the-art speckle reduction methods.},
author = {Wang, P and Zhang, H and Patel, V M},
booktitle = {2017 IEEE 7th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)},
doi = {10.1109/CAMSAP.2017.8313133},
file = {:Users/sergio/Documents/TFG/related papers/08313133.pdf:pdf},
keywords = {image denoising;image restoration;learning (artifi},
month = {dec},
pages = {1--5},
title = {{Generative adversarial network-based restoration of speckled SAR images}},
year = {2017}
}
@inproceedings{Han2017,
abstract = {Phase Contrast (PC) and Differential Interference Contrast (DIC) microscopy are two popular non-invasive techniques for monitoring live cells. Each of these two image modalities has its own advantages and disadvantages to visualize specimens, so biologists need these two complementary modalities together to analyze specimens. In this paper, we investigate a conditional Generative Adversarial Network (conditional GAN), which contains one generator and two discriminators, to transfer microscopy image modalities. Given a training dataset consisting of pairs of images (source and destination) captured on the same set of specimens by DIC and Phase Contrast microscopes, we can train a conditional GAN, and with this well-trained GAN, we can generate the corresponding Phase Contrast image given a new DIC image, vice versa. The preliminary experiments demonstrate that our approach outperforms one state-of-the-arts method, and can provide biologists a computational way to switch between microscopy image modalities, so biologists can combine the advantages of different image modalities to better visualize and analyze specimens over time, without purchasing all types of microscopy image modalities or switching between imaging systems back-andforth during time-lapse experiments.},
author = {Han, Liang and Yin, Zhaozheng},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2017.118},
file = {:Users/sergio/Documents/TFG/related papers/08014852.pdf:pdf},
isbn = {9781538607336},
issn = {21607516},
pages = {851--859},
title = {{Transferring Microscopy Image Modalities with Conditional Generative Adversarial Networks}},
volume = {2017-July},
year = {2017}
}


%% DEEP LEARNING.
% OG stuff.
@article{McCulloch1943,
author="McCulloch, Warren S. and Pitts, Walter",
title="A logical calculus of the ideas immanent in nervous activity",
journal="The bulletin of mathematical biophysics",
year="1943",
volume="5",
number="4",
pages="115--133",
abstract="Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.",
issn="1522-9602",
doi="10.1007/BF02478259",
url="https://doi.org/10.1007/BF02478259"
}
@article{Rosenblatt58theperceptron,
author={F. Rosenblatt},
title={The Perceptron: A Probabilistic Model for Information Storage and Organization in The Brain},
journal={Psychological Review},
year={1958},
pages={65--386}
}
@inbook{Rumelhart1987,
author={D. E. {Rumelhart} and J. L. {McClelland}},
booktitle={Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations},
title={Learning Internal Representations by Error Propagation},
year={1987},
abstract={This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion},
publisher={MITP},
url={https://ieeexplore.ieee.org/document/6302929}
}
@article{Cybenko1989,
author="Cybenko, G.",
title="Approximation by superpositions of a sigmoidal function",
journal="Mathematics of Control, Signals and Systems",
year="1989",
day="01",
volume="2",
number="4",
pages="303--314",
abstract="In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.",
issn="1435-568X",
doi="10.1007/BF02551274",
url="https://doi.org/10.1007/BF02551274"
}
@article{Hornik1991251,
title = "Approximation capabilities of multilayer feedforward networks",
journal = "Neural Networks",
volume = "4",
number = "2",
pages = "251 - 257",
year = "1991",
issn = "0893-6080",
doi = "https://doi.org/10.1016/0893-6080(91)90009-T",
url = "http://www.sciencedirect.com/science/article/pii/089360809190009T",
author = "Kurt Hornik",
keywords = "Multilayer feedforward networks, Activation function, Universal approximation capabilities, Input environment measure, () approximation, Uniform approximation, Sobolev spaces, Smooth approximation",
abstract = "We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives."
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
file = {:Users/sergio/Documents/TFG/related papers/00726791.pdf:pdf},
journal = {Proceedings of the IEEE},
keywords = {character recognition,convolutional neural networks,document recog-,finite state transducers,gradient-based learning,graph,machine learning,neural networks,nition,ocr,optical,transformer networks},
number = {11},
pages = {2278--2324},
title = {{Gradient-Based Learning Applied to Document Recognition}},
volume = {86},
year = {1998}
}
@inproceedings{Krizhevsky:2012:ICD:2999134.2999257,
address = {USA},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
file = {:Users/sergio/Documents/TFG/related papers/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf:pdf},
pages = {1097--1105},
publisher = {Curran Associates Inc.},
series = {NIPS'12},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://dl.acm.org/citation.cfm?id=2999134.2999257},
year = {2012}
}

% Visualization paper.
@inproceedings{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {arXiv:1311.2901v3},
author = {Zeiler, Matthew D and Fergus, Rob},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {arXiv:1311.2901v3},
file = {:Users/sergio/Documents/TFG/related papers/1311.2901.pdf:pdf},
isbn = {9783319105895},
issn = {16113349},
number = {PART 1},
pages = {818--833},
title = {{Visualizing and understanding convolutional networks}},
volume = {8689 LNCS},
year = {2014}
}

% Optimization.
@misc{Hinton2012RMSProp,
author = {Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
title = {Overview of mini-batch gradient descent},
year = {2012 (accessed September 14, 2019)},
howpublished = "\url{https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}"
}
@misc{Kingma2014Adam,
title={Adam: A Method for Stochastic Optimization},
author={Diederik P. Kingma and Jimmy Ba},
year={2014},
eprint={1412.6980},
archivePrefix={arXiv},
primaryClass={cs.LG}
}
@misc{Zeiler2012ADADELTA,
title={ADADELTA: An Adaptive Learning Rate Method},
author={Matthew D. Zeiler},
year={2012},
eprint={1212.5701},
archivePrefix={arXiv},
primaryClass={cs.LG}
}
@misc{Adagrad,
title={Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
author={Duchi, John and Hazan, Elad and Singer, Yoram},
year={2011},
journal={Journal of Machine Learning Research (JMLR)}
}

% Dilated convolution.
@article{Yu2015,
abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
archivePrefix = {arXiv},
arxivId = {1511.07122},
author = {Yu, Fisher and Koltun, Vladlen},
eprint = {1511.07122},
file = {:Users/sergio/Documents/TFG/related papers/1511.07122.pdf:pdf},
title = {{Multi-Scale Context Aggregation by Dilated Convolutions}},
url = {http://arxiv.org/abs/1511.07122},
year = {2015}
}

% U-net.
@inproceedings{Ronneberger2015,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-24574-4_28},
eprint = {1505.04597},
file = {:Users/sergio/Documents/TFG/related papers/1505.04597.pdf:pdf},
isbn = {9783319245737},
issn = {16113349},
pmid = {23285570},
title = {{U-net: Convolutional networks for biomedical image segmentation}},
year = {2015}
}
@article{Radford2015,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
eprint = {1511.06434},
file = {:Users/sergio/Library/Application Support/Mendeley Desktop/Downloaded/Radford, Metz, Chintala - 2015 - Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.pdf:pdf},
month = {nov},
title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1511.06434},
year = {2015}
}

% GANs stuff.
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2661v1},
author = {Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
doi = {10.1016/B978-0-408-00109-0.50001-8},
eprint = {arXiv:1406.2661v1},
file = {:Users/sergio/Documents/TFG/related papers/1406.2661.pdf:pdf},
isbn = {1406.2661},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 27},
pages = {1--9},
pmid = {15645445},
title = {{Generative Adversarial Nets}},
url = {https://arxiv.org/abs/1406.2661},
year = {2014}
}
@article{DBLP:journals/corr/SalimansGZCRC16,
archivePrefix = {arXiv},
arxivId = {1606.03498},
author = {Salimans, Tim and Goodfellow, Ian J and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
eprint = {1606.03498},
file = {:Users/sergio/Documents/TFG/related papers/1606.03498.pdf:pdf},
journal = {CoRR},
title = {{Improved Techniques for Training GANs}},
url = {http://arxiv.org/abs/1606.03498},
volume = {abs/1606.0},
year = {2016}
}
@article{DBLP:journals/corr/Goodfellow17,
archivePrefix = {arXiv},
arxivId = {1701.00160},
author = {Goodfellow, Ian J},
eprint = {1701.00160},
journal = {CoRR},
title = {{{\{}NIPS{\}} 2016 Tutorial: Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1701.00160},
volume = {abs/1701.0},
year = {2017}
}
@article{Arjovsky2017,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
archivePrefix = {arXiv},
arxivId = {1701.07875},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
eprint = {1701.07875},
file = {:Users/sergio/Documents/TFG/related papers/1701.07875.pdf:pdf},
title = {{Wasserstein GAN}},
url = {http://arxiv.org/abs/1701.07875},
year = {2017}
}
@inproceedings{miyato2018spectral,
author = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi and Networks, Preferred},
booktitle = {International Conference on Learning Representations},
file = {:Users/sergio/Documents/TFG/related papers/e7852ae61dc5192febbc9f41621787f33116f09d.pdf:pdf},
title = {{Spectral Normalization for Generative Adversarial Networks}},
url = {https://openreview.net/forum?id=B1QRgziT-},
year = {2018}
}

% Image-to-image translation.
@article{Isola2016,  % pix2pix
abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
archivePrefix = {arXiv},
arxivId = {1611.07004},
author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
eprint = {1611.07004},
file = {:Users/sergio/Library/Application Support/Mendeley Desktop/Downloaded/Isola et al. - 2016 - Image-to-Image Translation with Conditional Adversarial Networks.pdf:pdf},
month = {nov},
title = {{Image-to-Image Translation with Conditional Adversarial Networks}},
url = {http://arxiv.org/abs/1611.07004},
year = {2016}
}
@article{Zhu2017a,  % CycleGANs
abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain {\$}X{\$} to a target domain {\$}Y{\$} in the absence of paired examples. Our goal is to learn a mapping {\$}G: X \backslashrightarrow Y{\$} such that the distribution of images from {\$}G(X){\$} is indistinguishable from the distribution {\$}Y{\$} using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping {\$}F: Y \backslashrightarrow X{\$} and introduce a cycle consistency loss to push {\$}F(G(X)) \backslashapprox X{\$} (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
archivePrefix = {arXiv},
arxivId = {1703.10593},
author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
eprint = {1703.10593},
file = {:Users/sergio/Library/Application Support/Mendeley Desktop/Downloaded/Zhu et al. - 2017 - Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.pdf:pdf},
month = {mar},
title = {{Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks}},
url = {http://arxiv.org/abs/1703.10593},
year = {2017}
}
@inproceedings{Lin2018,
abstract = {Image-to-image translation tasks have been widely investigated with Generative Adversarial Networks (GANs) and dual learning. However, existing models lack the ability to control the translated results in the target domain and their results usually lack of diversity in the sense that a fixed image usually leads to (almost) deterministic translation result. In this paper, we study a new problem, conditional image-to-image translation, which is to translate an image from the source domain to the target domain conditioned on a given image in the target domain. It requires that the generated image should inherit some domain-specific features of the conditional image from the target domain. Therefore, changing the conditional image in the target domain will lead to diverse translation results for a fixed input image from the source domain, and therefore the conditional input image helps to control the translation results. We tackle this problem with unpaired data based on GANs and dual learning. We twist two conditional translation models (one translation from A domain to B domain, and the other one from B domain to A domain) together for inputs combination and reconstruction while preserving domain independent features. We carry out experiments on men's faces from-to women's faces translation and edges to shoes{\&}bags translations. The results demonstrate the effectiveness of our proposed method.},
author = {Lin, Jianxin and Xia, Yingce and Qin, Tao and Chen, Zhibo and Liu, Tie Yan},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00579},
isbn = {9781538664209},
issn = {10636919},
pages = {5524--5532},
title = {{Conditional Image-to-Image Translation}},
year = {2018}
}
@article{Zhu2017,
abstract = {Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a $\backslash$emph{\{}distribution{\}} of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.},
archivePrefix = {arXiv},
arxivId = {1711.11586},
author = {Zhu, Jun-Yan and Zhang, Richard and Pathak, Deepak and Darrell, Trevor and Efros, Alexei A. and Wang, Oliver and Shechtman, Eli},
eprint = {1711.11586},
file = {:Users/sergio/Library/Application Support/Mendeley Desktop/Downloaded/Zhu et al. - 2017 - Toward Multimodal Image-to-Image Translation.pdf:pdf},
month = {nov},
title = {{Toward Multimodal Image-to-Image Translation}},
url = {http://arxiv.org/abs/1711.11586},
year = {2017}
}
@article{Ma2018,
abstract = {Unsupervised image translation, which aims in translating two independent sets of images, is challenging in discovering the correct correspondences without paired data. Existing works build upon Generative Adversarial Network (GAN) such that the distribution of the translated images are indistinguishable from the distribution of the target set. However, such set-level constraints cannot learn the instance-level correspondences (e.g. aligned semantic parts in object configuration task). This limitation often results in false positives (e.g. geometric or semantic artifacts), and further leads to mode collapse problem. To address the above issues, we propose a novel framework for instance-level image translation by Deep Attention GAN (DA-GAN). Such a design enables DA-GAN to decompose the task of translating samples from two sets into translating instances in a highly-structured latent space. Specifically, we jointly learn a deep attention encoder, and the instancelevel correspondences could be consequently discovered through attending on the learned instance pairs. Therefore, the constraints could be exploited on both set-level and instance-level. Comparisons against several state-ofthe- arts demonstrate the superiority of our approach, and the broad application capability, e.g, pose morphing, data augmentation, etc., pushes the margin of domain translation problem.},
author = {Ma, Shuang and Fu, Jianlong and Chen, Chang Wen and Mei, Tao},
doi = {10.1109/CVPR.2018.00593},
file = {:Users/sergio/Documents/TFG/related papers/DA-GAN.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {5657--5666},
publisher = {IEEE},
title = {{DA-GAN: Instance-Level Image Translation by Deep Attention Generative Adversarial Networks}},
year = {2018}
}
@article{Lin2019,
abstract = {Humans can only interact with part of the surrounding environment due to biological restrictions. Therefore, we learn to reason the spatial relationships across a series of observations to piece together the surrounding environment. Inspired by such behavior and the fact that machines also have computational constraints, we propose $\backslash$underline{\{}CO{\}}nditional $\backslash$underline{\{}CO{\}}ordinate GAN (COCO-GAN) of which the generator generates images by parts based on their spatial coordinates as the condition. On the other hand, the discriminator learns to justify realism across multiple assembled patches by global coherence, local appearance, and edge-crossing continuity. Despite the full images are never generated during training, we show that COCO-GAN can produce $\backslash$textbf{\{}state-of-the-art-quality{\}} full images during inference. We further demonstrate a variety of novel applications enabled by teaching the network to be aware of coordinates. First, we perform extrapolation to the learned coordinate manifold and generate off-the-boundary patches. Combining with the originally generated full image, COCO-GAN can produce images that are larger than training samples, which we called "beyond-boundary generation". We then showcase panorama generation within a cylindrical coordinate system that inherently preserves horizontally cyclic topology. On the computation side, COCO-GAN has a built-in divide-and-conquer paradigm that reduces memory requisition during training and inference, provides high-parallelism, and can generate parts of images on-demand.},
archivePrefix = {arXiv},
arxivId = {1904.00284},
author = {Lin, Chieh Hubert and Chang, Chia-Che and Chen, Yu-Sheng and Juan, Da-Cheng and Wei, Wei and Chen, Hwann-Tzong},
eprint = {1904.00284},
file = {:Users/sergio/Documents/TFG/related papers/1904.00284.pdf:pdf},
title = {{COCO-GAN: Generation by Parts via Conditional Coordinating}},
url = {http://arxiv.org/abs/1904.00284},
year = {2019}
}


%% OTHERS.
@article{Tellez2018,
abstract = {IEEE Manual counting of mitotic tumor cells in tissue sections constitutes one of the strongest prognostic markers for breast cancer. This procedure, however, is time-consuming and error-prone. We developed a method to automatically detect mitotic figures in breast cancer tissue sections based on convolutional neural networks (CNNs). Application of CNNs to hematoxylin and eosin (H {\&} {\#}x0026;E) stained histological tissue sections is hampered by: (1) noisy and expensive reference standards established by pathologists, (2) lack of generalization due to staining variation across laboratories, and (3) high computational requirements needed to process gigapixel whole-slide images (WSIs). In this paper, we present a method to train and evaluate CNNs to specifically solve these issues in the context of mitosis detection in breast cancer WSIs. First, by combining image analysis of mitotic activity in phosphohistone-H3 (PHH3) restained slides and registration, we built a reference standard for mitosis detection in entire H {\&} {\#}x0026;E WSIs requiring minimal manual annotation effort. Second, we designed a data augmentation strategy that creates diverse and realistic H {\&} {\#}x0026;E stain variations by modifying the hematoxylin and eosin color channels directly. Using it during training combined with network ensembling resulted in a stain invariant mitosis detector. Third, we applied knowledge distillation to reduce the computational requirements of the mitosis detection ensemble with a negligible loss of performance. The system was trained in a single-center cohort and evaluated in an independent multicenter cohort from The Cancer Genome Atlas on the three tasks of the Tumor Proliferation Assessment Challenge (TUPAC). We obtained a performance within the top-3 best methods for most of the tasks of the challenge.},
author = {Tellez, David and Balkenhol, Maschenka and Otte-H{\"{o}}ller, Irene and {Van De Loo}, Rob and Vogels, Rob and Bult, Peter and Wauters, Carla and Vreuls, Willem and Mol, Suzanne and Karssemeijer, Nico and Litjens, Geert and {Van Der Laak}, Jeroen and Ciompi, Francesco},
doi = {10.1109/TMI.2018.2820199},
file = {:Users/sergio/Documents/TFG/related papers/08327641.pdf:pdf},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Breast cancer,convolutional neural networks,data augmentation,knowledge distillation,mitosis detection,phosphohistone-H3},
title = {{Whole-Slide Mitosis Detection in H{\&}E Breast Histology Using PHH3 as a Reference to Train Distilled Stain-Invariant Convolutional Networks}},
year = {2018}
}

% Confocal microscopy.
@inbook{Inoue2006,
author="Inou{\'e}, Shinya",
editor="Pawley, James B.",
title="Foundations of Confocal Scanned Imaging in Light Microscopy",
bookTitle="Handbook Of Biological Confocal Microscopy",
year="2006",
publisher="Springer US",
address="Boston, MA",
pages="1--19",
abstract="Seldom has the introduction of a new instrument generated as instant an excitement among biologists as the laser-scanning confocal microscope. With the new microscope, one can slice incredibly clean, thin optical sections out of thick fluorescent specimens; view specimens in planes tilted to, and even running parallel to, the line of sight; penetrate deep into light-scattering tissues; gain impressive three-dimensional (3D) views at very high resolution; obtain differential interference or phase-contrast images in exact register with confocal fluorescence images; and improve the precision of microphotometry.",
isbn="978-0-387-45524-2",
doi="10.1007/978-0-387-45524-2_1",
url="https://doi.org/10.1007/978-0-387-45524-2_1"
}
@article{Chung2005,
author = {Chung, Vinh and Dwyer, Peter and Nehal, Kishwer and Rajadhyaksha, Milind and Menaker, Gregg and Charles, Carlos and Jiang, Shang I Brian},
year = {2005},
month = {01},
pages = {1470-8},
title = {Use of Ex Vivo Confocal Scanning Laser Microscopy during Mohs Surgery for Nonmelanoma Skin Cancers},
volume = {30},
journal = {Dermatologic surgery : official publication for American Society for Dermatologic Surgery [et al.]},
doi = {10.1111/j.1524-4725.2004.30505.x}
}
@article{Skvara2012,
author = {Skvara, Hans and Plut, Ulrike and Schmid, Johannes A and Jonak, Constanze},
year = {2012},
pages = {3-12},
title = {Combining in vivo reflectance with fluorescence confocal microscopy provides},
journal = {Dermatology Practical \& Conceptual},
doi = {10.5826/dpc.0201a02}
}
@article{Cinotti2018,
author = {Cinotti, Elisa and Perrot, Jean and Labeille, Bruno and Cambazard, Frédéric and Rubegni, Pietro},
year = {2018},
month = {04},
pages = {109-119},
title = {Ex vivo confocal microscopy: an emerging technique in dermatology},
volume = {8},
journal = {Dermatology Practical \& Conceptual},
doi = {10.5826/dpc.0802a08}
}
@inproceedings{Gigilashvili2018,
abstract = {Speckle noise is a form of multiplicative noise that corrupts the quality of medical images. It is well described and studied in medical ultrasound imaging, but less attention has been paid to its presence in reflectance microscopy images. Presence of the speckle noise not only limits the application of further post-processing and computer vision techniques, like edge detection, but it also makes diagnostics more difficult and less reliable for physicians. Many speckle mitigation techniques have been studied by various researchers, but the vast majority of them limit themselves to a single image of the target tissue. In this study, we examined the possibility to mitigate speckle using the redundant data present in consecutive frames of the video recordings, as they provide uncorrelated data from different spatial positions. Different ways of processing the redundant data were examined and compared against conventional methodologies.},
author = {Gigilashvili, D and Yin, Chengbo and Liu, J T C and Hardeberg, J Y and Pedersen, M},
booktitle = {2018 14th International Conference on Signal-Image Technology {\&} Internet-Based Systems (SITIS)},
doi = {10.1109/SITIS.2018.00050},
file = {:Users/sergio/Documents/TFG/related papers/08706173.pdf:pdf},
isbn = {VO  -},
keywords = {Biomedical imaging,Confocal microscopy,Denoising,Despeckling,Filtering,Image processing,Indexes,Medical imaging,Microscopy,Speckle,Three-dimensional displays,Two dimensional displays,biological tissues,biomedical optical imaging,biomedical ultrasonics,computer vision,computer vision techniques,dual-axis confocal microscopy,edge detection,image denoising,medical image processing,medical images,medical ultrasound imaging,optical microscopy,reflectance microscopy images,speckle,speckle mitigation techniques,speckle noise},
pages = {281--288},
publisher = {IEEE},
title = {{Measuring and Mitigating Speckle Noise in Dual-Axis Confocal Microscopy Images}},
year = {2018}
}

% SSIM paper
@article{Wang2004,
abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
author = {Wang, Zhou and Bovik, A C and Sheikh, H R and Simoncelli, E P},
doi = {10.1109/TIP.2003.819861},
file = {:Users/sergio/Documents/TFG/related papers/wang03-reprint.pdf:pdf},
issn = {1057-7149 VO - 13},
journal = {IEEE Transactions on Image Processing},
keywords = {Algorithms,Automated,Computer-Assisted,Data Interpretation,Data mining,Degradation,Humans,Hypermedia,Image Enhancement,Image Interpretation,Image quality,Indexes,Information Storage and Retrieval,JPEG,JPEG2000,Layout,Models,Pattern Recognition,Quality Control,Quality assessment,Reproducibility of Results,Sensitivity and Specificity,Signal Processing,Statistical,Subtraction Technique,Transform coding,Visual perception,Visual system,data compression,distorted image,error sensitivity,error visibility,human visual perception,human visual system,image coding,image compression,image database,perceptual image quality assessment,reference image,structural information,structural similarity index,visual perception},
number = {4},
pages = {600--612},
title = {{Image quality assessment: from error visibility to structural similarity}},
url = {http://www.cns.nyu.edu/pub/lcv/wang03-reprint.pdf},
volume = {13},
year = {2004}
}

